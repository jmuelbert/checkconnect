# SPDX-License-Identifier: EUPL-1.2
#
# SPDX-FileCopyrightText: © 2025-present Jürgen Mülbert

from __future__ import annotations

from pathlib import Path
from typing import TYPE_CHECKING
from unittest.mock import patch

import pytest
from pydantic import ValidationError

import checkconnect.reports.report_generator as report_generator_module
from checkconnect.exceptions import DirectoryCreationError
from checkconnect.reports.report_generator import (
    ReportGenerator,
    ReportInput,
    ReportTemplate,
    generate_html_report,
    generate_pdf_report,
    generate_reports,
)

if TYPE_CHECKING:
    from pytest_mock import MockerFixture
    from structlog.typing import EventDict

    from checkconnect.config.appcontext import AppContext


@pytest.fixture
def report_generator_from_params_instance(app_context_fixture: AppContext, tmp_path: Path) -> ReportGenerator:
    """
    Fixture for a `ReportGenerator` instance created via `from_params`.

    This fixture provides a `ReportGenerator` instance initialized with a
    specific output directory, simulating the `from_params` factory method.

    Args:
    ----
        app_context_fixture: A pytest fixture providing the application context.
        tmp_path: A pytest fixture providing a temporary directory path.

    Returns:
    -------
        A `ReportGenerator` instance.
    """
    return ReportGenerator.from_params(context=app_context_fixture, arg_reports_dir=tmp_path / "output_from_params")


@pytest.fixture
def report_generator_from_context_instance(app_context_fixture: AppContext) -> ReportGenerator:
    """
    Fixture for a `ReportGenerator` instance created via `from_context`.

    This fixture provides a `ReportGenerator` instance initialized using
    the application context, which determines the output directory from settings.

    Args:
    ----
        app_context_fixture: A pytest fixture providing the application context.

    Returns:
    -------
        A `ReportGenerator` instance.
    """
    return ReportGenerator.from_context(context=app_context_fixture)


class TestReportTemplate:
    """
    Test suite for the `ReportTemplate` class.

    This class contains unit tests to verify the HTML rendering capabilities
    of the `ReportTemplate`, including default rendering and custom title/header
    configurations.
    """

    @pytest.mark.unit
    def test_default_rendering(self) -> None:
        """
        Test rendering with default template values.

        Ensures that the HTML output includes default titles, headers,
        and correctly embeds the provided content within `<pre>` tags.
        """
        template = ReportTemplate()
        ntp_content = "NTP Test Result 1\nNTP Test Result 2"
        url_content = "URL Test Result 1\nURL Test Result 2"
        html = template.render(ntp_content, url_content)

        assert "CheckConnect Report" in html
        assert "NTP Results" in html
        assert "URL Results" in html
        assert "NTP Test Result 1" in html
        assert "URL Test Result 1" in html
        assert "<pre>NTP Test Result 1" in html  # Check if content is in <pre>
        assert "<pre>URL Test Result 1" in html
        assert "Generated by CheckConnect" in html

    @pytest.mark.unit
    def test_custom_titles(self) -> None:
        """
        Test rendering with custom titles and headers.

        Verifies that `ReportTemplate` correctly incorporates user-defined
        `title`, `ntp_header`, and `url_header` into the generated HTML.
        """
        template = ReportTemplate(title="My Custom Report", ntp_header="My NTP Header", url_header="My URL Header")
        ntp_content = "NTP Content"
        url_content = "URL Content"
        html = template.render(ntp_content, url_content)

        assert "My Custom Report" in html
        assert "My NTP Header" in html
        assert "My URL Header" in html
        assert "NTP Content" in html
        assert "URL Content" in html


class TestReportInput:
    """
    Test suite for the `ReportInput` Pydantic model.

    This class contains unit tests to validate the data input model for reports,
    ensuring correct type enforcement and handling of empty or invalid data.
    """

    @pytest.mark.unit
    def test_valid_input(self) -> None:
        """
        Test `ReportInput` with valid data.

        Ensures that a `ReportInput` instance can be successfully created
        with well-formed NTP and URL results.
        """
        data = ReportInput(ntp_results=["ntp1"], url_results=["url1"])
        assert data.ntp_results == ["ntp1"]
        assert data.url_results == ["url1"]

    @pytest.mark.unit
    def test_empty_ntp_results_raises_error(self) -> None:
        """
        Test that an empty NTP results list raises a `ValidationError`.

        Verifies the Pydantic validation rule for non-empty `ntp_results`.
        """
        with pytest.raises(ValidationError) as excinfo:
            ReportInput(ntp_results=[], url_results=["url1"])
        # Check for the specific Pydantic error message
        assert any("Field 'ntp_results' cannot be empty." in error["msg"] for error in excinfo.value.errors())

    @pytest.mark.unit
    def test_empty_url_results_raises_error(self) -> None:
        """
        Test that an empty URL results list raises a `ValidationError`.

        Verifies the Pydantic validation rule for non-empty `url_results`.
        """
        with pytest.raises(ValidationError) as excinfo:
            ReportInput(ntp_results=["ntp1"], url_results=[])
        # Check for the specific Pydantic error message
        assert any("Field 'url_results' cannot be empty." in error["msg"] for error in excinfo.value.errors())

    @pytest.mark.unit
    def test_non_list_inputs_results_raises_validation_error(self) -> None:
        """
        Test that non-list inputs for results raise a `ValidationError`
        with the custom message from `ensure_list_of_strings`.
        """
        # Test case 1: ntp_results is not a list
        with pytest.raises(ValidationError) as excinfo:
            ReportInput(ntp_results="not a list", url_results=["url1"])

        error_found = False
        for error in excinfo.value.errors():
            # Expect the custom error message and 'value_error' type
            if "Input must be a list." in error["msg"] and error["type"] == "value_error":
                error_found = True
                break
        assert error_found, f"Expected 'Input must be a list.' (type value_error) but got: {excinfo.value.errors()}"

        # Test case 2: url_results is not a list
        with pytest.raises(ValidationError) as excinfo:
            ReportInput(ntp_results=["ntp1"], url_results="not a list")
        error_found = False
        for error in excinfo.value.errors():
            # Expect the custom error message and 'value_error' type
            if "Input must be a list." in error["msg"] and error["type"] == "value_error":
                error_found = True
                break
        assert error_found, f"Expected 'Input must be a list.' (type value_error) but got: {excinfo.value.errors()}"

    @pytest.mark.unit
    def test_list_with_non_string_items_raises_validation_error(self) -> None:
        """
        Test that lists containing non-string items raise a `ValidationError`.

        Verifies that the custom validator (`ensure_list_of_strings`) correctly
        flags lists with mixed or incorrect item types.
        """
        # Test case 1: ntp_results contains non-string elements
        with pytest.raises(ValidationError) as excinfo:
            ReportInput(ntp_results=["ntp1", 123], url_results=["url1"])
        assert any("All items in the list must be strings." in error["msg"] for error in excinfo.value.errors())
        assert any(error["type"] == "value_error" for error in excinfo.value.errors())

        # Test case 2: url_results contains non-string elements
        with pytest.raises(ValidationError) as excinfo:
            ReportInput(ntp_results=["ntp1"], url_results=["url1", None])
        assert any("All items in the list must be strings." in error["msg"] for error in excinfo.value.errors())
        assert any(error["type"] == "value_error" for error in excinfo.value.errors())


class TestReportGenerator:
    """
    Test suite for the `ReportGenerator` class.

    This class contains unit tests to verify the functionality of the `ReportGenerator`,
    including its initialization, directory management, and the generation of
    plain text, HTML, and PDF reports.
    """

    @pytest.mark.unit
    @pytest.mark.parametrize("app_context_fixture", ["simple"], indirect=True)
    def test_from_params_uses_explicit_reports_dir(self, app_context_fixture: AppContext, tmp_path: Path) -> None:
        """
        Test that `from_params` uses the default directory when none is configured.

        Ensures that if the `reports.directory` setting is missing from the config,
        the `ReportGenerator` falls back to its predefined default output path.
        """
        generator_reports_dir = tmp_path / "another_test_reports_dir"

        generator = ReportGenerator.from_params(context=app_context_fixture, arg_reports_dir=generator_reports_dir)

        assert generator.reports_dir == generator_reports_dir

    @pytest.mark.unit
    @pytest.mark.parametrize("app_context_fixture", ["full"], indirect=True)
    def test_from_context_uses_configured_reports_dir(
        self, report_generator_from_context_instance: ReportGenerator, tmp_path: Path
    ) -> None:
        """
        Test that `ReportGenerator.from_context` uses the configured directory when the context is 'full'.
        """
        app_context = report_generator_from_context_instance.context

        generator = ReportGenerator.from_context(context=app_context)

        expected_path_from_config = tmp_path / "test_reports_from_config"

        assert generator.reports_dir == expected_path_from_config

    @pytest.mark.unit
    @pytest.mark.parametrize("app_context_fixture", ["simple"], indirect=True)
    def test_from_context_uses_default_reports_dir_if_none_in_config(
        self,
        app_context_fixture: AppContext,  # Use this directly, no need for the report_generator_from_context_instance fixture if you're creating it here
        mocker,  # Use mocker from pytest-mock for patching
        caplog_structlog: list[EventDict],  # To capture structlog output
    ) -> None:
        """
        Test that `from_context` uses the default user data directory
        when the configuration does not specify a reports directory.

        Also asserts that the correct log messages are emitted.
        """
        # 1. Arrange (Setup)
        # Ensure config.settings.get returns None for "reports", "directory"
        # The 'simple' app_context_fixture should already have an empty config for this.
        # If not, you might need to mock context.settings.get specifically.
        mocker.patch.object(app_context_fixture.settings, "get", return_value=None)

        # Mock platformdirs.user_data_dir to return a predictable path for testing
        # This prevents creating real directories and ensures test reproducibility across OSes.
        mock_user_data_dir_path = Path("/mocked/user/data/reports/checkconnect")
        mocker.patch(f"{report_generator_module.__name__}.user_data_dir", return_value=str(mock_user_data_dir_path))

        # Mock Path.mkdir since ReportGenerator.__init__ calls it
        # We want to ensure it's called, but not actually create a directory.
        # We'll use a MagicMock for the return value of Path().mkdir()
        mock_mkdir = mocker.patch.object(Path, "mkdir", return_value=None)  # mkdir typically returns None

        # 2. Act (Call the code under test)
        generator = ReportGenerator.from_context(context=app_context_fixture)

        # 3. Assert (Verify behavior)

        # Assert that the reports_dir is the expected default path
        assert generator.reports_dir == mock_user_data_dir_path

        # Assert that mkdir was called correctly
        # It should be called on the Path object representing the default path.
        # The `parents=True` and `exist_ok=True` arguments are important.
        mock_mkdir.assert_called_once_with(parents=True, exist_ok=True)

        assert any(
            e.get("event") == "[mocked] Report directory not found in config or invalid. Using default."
            and e.get("path") == str(mock_user_data_dir_path)
            and e.get("log_level") == "warning"
            for e in caplog_structlog
        )

        assert any(
            e.get("event") == "[mocked] Ensured report directory exists."
            and e.get("path") == str(mock_user_data_dir_path)
            and e.get("log_level") == "debug"
            for e in caplog_structlog
        )

    @pytest.mark.unit
    @pytest.mark.parametrize("app_context_fixture", ["simple"], indirect=True)
    def test_ensure_reports_directory_raises_error_on_failure(
        self, app_context_fixture: AppContext, caplog_structlog: list[EventDict]
    ) -> None:
        """
        Test that `_ensure_reports_directory` raises `DirectoryCreationError` on failure
        and logs the appropriate error with structlog.

        Simulates an `OSError` during directory creation (e.g., permission issues)
        and asserts that the custom `DirectoryCreationError` is raised with
        appropriate messaging, and that the `structlog` output is correct.
        """
        # Define a target path that will definitely cause an OSError
        target_path = Path("/nonexistent/path_unwritable")

        # Define the expected error message from the OSError
        os_error_message = "Permission denied"

        with (
            # Patch Path.mkdir to simulate an OSError
            patch.object(Path, "mkdir", side_effect=OSError(os_error_message)),
            # Assert that DirectoryCreationError is raised
            pytest.raises(DirectoryCreationError) as excinfo,
        ):
            # Call the method that internally tries to create the directory
            ReportGenerator.from_params(context=app_context_fixture, arg_reports_dir=target_path)

        # Assertions for the raised exception
        assert f"Failed to create report directory: '{target_path}': {os_error_message}" in str(excinfo.value)
        assert os_error_message in str(excinfo.value)
        assert isinstance(excinfo.value.__cause__, OSError)

        assert any(
            e.get("event") == "[mocked] Failed to create report directory."
            and e.get("path") == str(target_path)
            and e.get("log_level") == "error"
            for e in caplog_structlog
        )

    @pytest.mark.unit
    @pytest.mark.parametrize("app_context_fixture", ["full"], indirect=True)
    def test_generate_report_creates_file(self, report_generator_from_context_instance: ReportGenerator) -> None:
        """
        Test the `generate_report` method, which creates a plain text file.

        Verifies that a report file is created at the specified path, contains
        the expected content (NTP and URL results), and that the logger
        is appropriately informed.
        """
        ntp_data = ["ntp1.example.com: OK", "ntp2.example.com: FAILED"]
        url_data = ["https://example.com: 200", "https://bad.com: 404"]
        report_input = ReportInput(ntp_results=ntp_data, url_results=url_data)

        report_filename = "my_connectivity_report.txt"
        report_path = report_generator_from_context_instance.generate_report(report_input, report_filename)

        expected_path = report_generator_from_context_instance.reports_dir / report_filename
        assert report_path == expected_path
        assert report_path.exists()
        assert report_path.is_file()

        content = report_path.read_text(encoding="utf-8")

        assert "[mocked] Connectivity Report" in content
        assert "[mocked] NTP Results:" in content
        assert "ntp1.example.com: OK" in content
        assert "NTP Test Result 2" not in content  # Ensure no old content is present
        assert "[mocked] URL Results:" in content
        assert "https://example.com: 200" in content

    @pytest.mark.unit
    def test_generate_html_report_success(
        self,
        report_generator_from_context_instance: ReportGenerator,
        mocker: MockerFixture,
        caplog_structlog: list[EventDict],
    ) -> None:
        """
        Test the successful creation of an HTML report.

        Verifies that `generate_html_report` correctly renders HTML using `ReportTemplate`,
        writes it to the expected file, and logs the successful generation.
        """
        ntp_data = ["ntp.example.com: OK"]
        url_data = ["https://example.com: 200"]

        # Mock ReportTemplate to control its rendering output
        mock_template_instance = mocker.MagicMock(spec=ReportTemplate)
        mock_template_instance.render.return_value = "<html>Mocked HTML</html>"
        mocker.patch("checkconnect.reports.report_generator.ReportTemplate", return_value=mock_template_instance)

        # Mock Path.write_text to prevent actual file system writes during this test
        mocker.patch.object(Path, "write_text")

        report_path = report_generator_from_context_instance.generate_html_report(ntp_data, url_data)

        expected_path = report_generator_from_context_instance.reports_dir / ReportGenerator.HTML_FILENAME
        assert report_path == expected_path
        # Assert ReportTemplate.render was called with the correct data
        mock_template_instance.render.assert_called_once_with("\n".join(ntp_data), "\n".join(url_data))
        # Assert Path.write_text was called with the rendered HTML and correct path
        Path.write_text.assert_called_once_with("<html>Mocked HTML</html>", encoding="utf-8")

        # Assert the logger was informed
        assert any(
            log_entry.get("event") == "[mocked] HTML reports generated." and log_entry.get("log_level") == "info"
            for log_entry in caplog_structlog
        )

    @pytest.mark.unit
    def test_generate_html_report_missing_data_raises_error(
        self,
        report_generator_from_context_instance: ReportGenerator,
        caplog_structlog: list[EventDict],
    ) -> None:
        """
        Test that `generate_html_report` raises `ValidationError` when no data is provided.

        Ensures that an attempt to generate an HTML report with empty NTP and URL
        result lists results in a `ValidationError` due to `ReportInput`'s validation.
        """

        with pytest.raises(ValidationError) as excinfo:  # Pydantic ValidationError
            report_generator_from_context_instance.generate_html_report([], [])

        assert "Field 'ntp_results' cannot be empty." in str(excinfo.value)
        assert "Field 'url_results' cannot be empty." in str(excinfo.value)

        assert any(
            "[mocked] Invalid report data." in log_entry.get("event") and log_entry.get("log_level") == "error"
            for log_entry in caplog_structlog
        )

    @pytest.mark.unit
    def test_generate_html_report_io_error(
        self,
        report_generator_from_context_instance: ReportGenerator,
        mocker: MockerFixture,
        caplog_structlog: list[EventDict],
    ) -> None:
        """
        Test that `generate_html_report` raises `OSError` when file writing fails.

        Simulates an `OSError` during the file writing process and asserts
        that `OSError` is propagated, and the logger records an exception.
        """
        ntp_data = ["ntp.example.com: OK"]
        url_data = ["https://example.com: 200"]

        # Mock ReportTemplate so it doesn't interfere with this specific test
        mocker.patch(
            "checkconnect.reports.report_generator.ReportTemplate",
            return_value=mocker.MagicMock(
                spec=ReportTemplate, render=mocker.MagicMock(return_value="<html>Mocked Report Content</html>")
            ),
        )

        # Create an actual OSError instance to be raised and asserted against
        mock_os_error_instance = OSError("Disk full: No space left on device")

        # Mock pathlib.Path.write_text to raise an OSError when called
        mocker.patch(
            "pathlib.Path.write_text",
            side_effect=mock_os_error_instance,
        )

        with pytest.raises(OSError, match="Disk full: No space left on device"):
            report_generator_from_context_instance.generate_html_report(ntp_data, url_data)

        assert any(
            "[mocked] Error generating HTML report." in e.get("event") and e.get("log_level") == "error"
            for e in caplog_structlog
        )

    @pytest.mark.unit
    def test_generate_pdf_report_success(
        self,
        report_generator_from_context_instance: ReportGenerator,
        mocker: MockerFixture,
        caplog_structlog: list[EventDict],
    ) -> None:
        """
        Test the successful creation of a PDF report.

        Verifies that `generate_pdf_report` correctly leverages `ReportTemplate`
        and `weasyprint.HTML` to generate a PDF, and logs the success.
        """
        ntp_data = ["ntp.example.com: OK"]
        url_data = ["https://example.com: 200"]

        # Mock ReportTemplate
        mock_template_instance = mocker.MagicMock(spec=ReportTemplate)
        mock_template_instance.render.return_value = "<html>Mocked HTML for PDF</html>"
        mocker.patch("checkconnect.reports.report_generator.ReportTemplate", return_value=mock_template_instance)

        # IMPORTANT: Mock the HTML class from the module where it's imported.
        # Here, that's the `checkconnect.reports.report_generator` module itself.
        mock_weasyprint_html_class = mocker.MagicMock()
        mock_weasyprint_html_instance = mocker.MagicMock()
        # When HTML() is called, it should return mock_weasyprint_html_instance
        mock_weasyprint_html_class.return_value = mock_weasyprint_html_instance
        mocker.patch("checkconnect.reports.report_generator.HTML", new=mock_weasyprint_html_class)

        report_path = report_generator_from_context_instance.generate_pdf_report(ntp_data, url_data)

        expected_path = report_generator_from_context_instance.reports_dir / ReportGenerator.PDF_FILENAME
        assert report_path == expected_path

        # Assert ReportTemplate.render was called correctly
        mock_template_instance.render.assert_called_once_with("\n".join(ntp_data), "\n".join(url_data))
        # Assert weasyprint.HTML (the mocked constructor) was called with the rendered HTML
        mock_weasyprint_html_class.assert_called_once_with(string="<html>Mocked HTML for PDF</html>")
        # Assert write_pdf was called on the HTML instance
        mock_weasyprint_html_instance.write_pdf.assert_called_once_with(str(expected_path))

        assert any(
            "[mocked] PDF reports generated." in e.get("event")
            and e.get("path") == str(expected_path)
            and e.get("log_level") == "info"
            for e in caplog_structlog
        )

    @pytest.mark.unit
    def test_generate_pdf_report_missing_data_raises_error(
        self,
        report_generator_from_context_instance: ReportGenerator,
        caplog_structlog: list[EventDict],
    ) -> None:
        """
        Test that `generate_pdf_report` raises `ValidationError` when no data is provided.

        Ensures that an attempt to generate a PDF report with empty NTP and URL
        result lists results in a `ValidationError` due to `ReportInput`'s validation.
        """
        with pytest.raises(ValidationError) as excinfo:  # Pydantic ValidationError
            report_generator_from_context_instance.generate_pdf_report([], [])

        assert "Field 'ntp_results' cannot be empty." in str(excinfo.value)
        assert "Field 'url_results' cannot be empty." in str(excinfo.value)
        assert (
            "[mocked] Invalid report data: 2 validation errors for ReportInput\nntp_results\n" in e.get("event")
            and e.get("log_level") == "error"
            for e in caplog_structlog
        )

    @pytest.mark.unit
    def test_generate_pdf_report_error(
        self,
        mocker: MockerFixture,
        report_generator_from_context_instance: ReportGenerator,
        caplog_structlog: list[EventDict],
    ) -> None:
        """
        Test that `generate_pdf_report` raises a general `Exception` on PDF generation failure.

        Simulates an arbitrary `Exception` during the `weasyprint` PDF generation
        process and asserts that the `Exception` is propagated, and the logger
        records an exception.
        """
        ntp_data = ["ntp.example.com: OK"]
        url_data = ["https://example.com: 200"]

        # Mock ReportTemplate
        mocker.patch(
            "checkconnect.reports.report_generator.ReportTemplate", return_value=mocker.MagicMock(spec=ReportTemplate)
        )
        # Mock WeasyPrint HTML and make write_pdf raise an error
        mock_weasyprint_html_instance = mocker.MagicMock()
        mock_weasyprint_html_instance.__str__.return_value = "WeasyPrint error"
        mock_weasyprint_html_instance.write_pdf.side_effect = Exception("WeasyPrint error")
        mocker.patch("checkconnect.reports.report_generator.HTML", return_value=mock_weasyprint_html_instance)

        with pytest.raises(Exception, match="WeasyPrint error"):  # WeasyPrint raises generic Exception
            report_generator_from_context_instance.generate_pdf_report(ntp_data, url_data)

        assert any(
            e.get("event") == "[mocked] Error generating PDF report." and e.get("log_level") == "error"
            for e in caplog_structlog
        )

    @pytest.mark.unit
    @pytest.mark.parametrize("app_context_fixture", ["simple"], indirect=True)
    def test_generate_reports_calls_both_html_and_pdf(
        self,
        report_generator_from_context_instance: ReportGenerator,
        mocker: MockerFixture,
        caplog_structlog: list[EventDict],
    ) -> None:
        """
        Test that `generate_reports` calls both `generate_html_report` and `generate_pdf_report`.

        Ensures that the orchestrating `generate_reports` method correctly
        invokes its sub-methods for both HTML and PDF generation with the
        provided data.
        """
        ntp_data = ["ntp data"]
        url_data = ["url data"]

        # Mock the internal methods to check if they are called
        mocker.patch.object(report_generator_from_context_instance, "generate_html_report")
        mocker.patch.object(report_generator_from_context_instance, "generate_pdf_report")

        report_generator_from_context_instance.generate_reports(ntp_data, url_data)

        # Assert both methods were called with the correct arguments
        report_generator_from_context_instance.generate_html_report.assert_called_once_with(
            ntp_results=ntp_data, url_results=url_data
        )
        report_generator_from_context_instance.generate_pdf_report.assert_called_once_with(
            ntp_results=ntp_data, url_results=url_data
        )
        assert any(
            event.get("event") == "[mocked] Generating reports"
            and event.get("path") == str(report_generator_from_context_instance.reports_dir)
            and event.get("log_level") == "info"
            for event in caplog_structlog
        )

    @pytest.mark.unit
    @pytest.mark.parametrize("app_context_fixture", ["simple"], indirect=True)
    def test_module_level_generate_html_report(self, app_context_fixture: AppContext, mocker: MockerFixture) -> None:
        """
        Test the module-level `generate_html_report` function.

        Verifies that the convenience function at the module level correctly
        instantiates `ReportGenerator` and delegates to its `generate_html_report` method.
        """

        ntp_data = ["mod ntp"]
        url_data = ["mod url"]
        mock_report_generator_instance = mocker.Mock()
        mock_from_context = mocker.patch(
            "checkconnect.reports.report_generator.ReportGenerator.from_context",
            return_value=mock_report_generator_instance,
        )

        generate_html_report(app_context_fixture, ntp_data, url_data)

        # Assert: from_context was called with correct context
        mock_from_context.assert_called_once_with(context=app_context_fixture)

        mock_report_generator_instance.generate_html_report.assert_called_once_with(ntp_data, url_data)

    @pytest.mark.unit
    @pytest.mark.parametrize("app_context_fixture", ["simple"], indirect=True)
    def test_module_level_generate_pdf_report(self, app_context_fixture: AppContext, mocker: MockerFixture) -> None:
        """
        Test the module-level `generate_pdf_report` function.

        Verifies that the convenience function correctly instantiates ReportGenerator
        using from_context and calls its generate_pdf_report method.
        """

        ntp_data = ["mod ntp"]
        url_data = ["mod url"]

        # Patch from_context in the correct module
        mock_report_generator_instance = mocker.Mock()
        mock_from_context = mocker.patch(
            "checkconnect.reports.report_generator.ReportGenerator.from_context",
            return_value=mock_report_generator_instance,
        )

        # Act
        generate_pdf_report(app_context_fixture, ntp_data, url_data)

        # Assert: from_context was called with correct context
        mock_from_context.assert_called_once_with(context=app_context_fixture)

        # Assert: generate_pdf_report called with correct data
        mock_report_generator_instance.generate_pdf_report.assert_called_once_with(ntp_data, url_data)

    @pytest.mark.unit
    @pytest.mark.parametrize("app_context_fixture", ["simple"], indirect=True)
    def test_module_level_generate_reports(self, app_context_fixture: AppContext, mocker: MockerFixture) -> None:
        """
        Test that generate_reports() calls both HTML and PDF generation with the correct data.
        """
        # Arrange
        ntp_data = ["ntp1", "ntp2"]
        url_data = ["url1", "url2"]

        mock_report_generator_instance = mocker.Mock()
        mock_from_context = mocker.patch(
            "checkconnect.reports.report_generator.ReportGenerator.from_context",
            return_value=mock_report_generator_instance,
        )

        # Act
        generate_reports(app_context_fixture, ntp_data, url_data)

        # Assert: from_context called with correct context
        mock_from_context.assert_called_once_with(context=app_context_fixture)

        # Assert: both generate_* methods called correctly
        mock_report_generator_instance.generate_html_report.assert_called_once_with(ntp_data, url_data)
        mock_report_generator_instance.generate_pdf_report.assert_called_once_with(ntp_data, url_data)
