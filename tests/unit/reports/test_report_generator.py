# SPDX-License-Identifier: EUPL-1.2
#
# SPDX-FileCopyrightText: © 2025-present Jürgen Mülbert

from __future__ import annotations

from pathlib import Path
from typing import TYPE_CHECKING
from unittest.mock import patch

import pytest
from pydantic import ValidationError

from checkconnect.exceptions import DirectoryCreationError
from checkconnect.reports.report_generator import (
    ReportGenerator,
    ReportInput,
    ReportTemplate,
    generate_html_report,
    generate_pdf_report,
    generate_reports,
)

if TYPE_CHECKING:
    from pytest_mock import MockerFixture
    from structlog.typing import EventDict

    from checkconnect.config.appcontext import AppContext


@pytest.fixture
def report_generator_from_params_instance(app_context_fixture: AppContext, tmp_path: Path) -> ReportGenerator:
    """
    Fixture for a `ReportGenerator` instance created via `from_params`.

    This fixture provides a `ReportGenerator` instance initialized with a
    specific output directory, simulating the `from_params` factory method.

    Args:
    ----
        app_context_fixture: A pytest fixture providing the application context.
        tmp_path: A pytest fixture providing a temporary directory path.

    Returns:
    -------
        A `ReportGenerator` instance.
    """
    return ReportGenerator.from_params(context=app_context_fixture, reports_dir=tmp_path / "output_from_params")


@pytest.fixture
def report_generator_from_context_instance(app_context_fixture: AppContext) -> ReportGenerator:
    """
    Fixture for a `ReportGenerator` instance created via `from_context`.

    This fixture provides a `ReportGenerator` instance initialized using
    the application context, which determines the output directory from settings.

    Args:
    ----
        app_context_fixture: A pytest fixture providing the application context.

    Returns:
    -------
        A `ReportGenerator` instance.
    """
    return ReportGenerator.from_context(context=app_context_fixture)


class TestReportTemplate:
    """
    Test suite for the `ReportTemplate` class.

    This class contains unit tests to verify the HTML rendering capabilities
    of the `ReportTemplate`, including default rendering and custom title/header
    configurations.
    """

    @pytest.mark.unit
    def test_default_rendering(self) -> None:
        """
        Test rendering with default template values.

        Ensures that the HTML output includes default titles, headers,
        and correctly embeds the provided content within `<pre>` tags.
        """
        template = ReportTemplate()
        ntp_content = "NTP Test Result 1\nNTP Test Result 2"
        url_content = "URL Test Result 1\nURL Test Result 2"
        html = template.render(ntp_content, url_content)

        assert "CheckConnect Report" in html
        assert "NTP Results" in html
        assert "URL Results" in html
        assert "NTP Test Result 1" in html
        assert "URL Test Result 1" in html
        assert "<pre>NTP Test Result 1" in html  # Check if content is in <pre>
        assert "<pre>URL Test Result 1" in html
        assert "Generated by CheckConnect" in html

    @pytest.mark.unit
    def test_custom_titles(self) -> None:
        """
        Test rendering with custom titles and headers.

        Verifies that `ReportTemplate` correctly incorporates user-defined
        `title`, `ntp_header`, and `url_header` into the generated HTML.
        """
        template = ReportTemplate(title="My Custom Report", ntp_header="My NTP Header", url_header="My URL Header")
        ntp_content = "NTP Content"
        url_content = "URL Content"
        html = template.render(ntp_content, url_content)

        assert "My Custom Report" in html
        assert "My NTP Header" in html
        assert "My URL Header" in html
        assert "NTP Content" in html
        assert "URL Content" in html


class TestReportInput:
    """
    Test suite for the `ReportInput` Pydantic model.

    This class contains unit tests to validate the data input model for reports,
    ensuring correct type enforcement and handling of empty or invalid data.
    """

    @pytest.mark.unit
    def test_valid_input(self) -> None:
        """
        Test `ReportInput` with valid data.

        Ensures that a `ReportInput` instance can be successfully created
        with well-formed NTP and URL results.
        """
        data = ReportInput(ntp_results=["ntp1"], url_results=["url1"])
        assert data.ntp_results == ["ntp1"]
        assert data.url_results == ["url1"]

    @pytest.mark.unit
    def test_empty_ntp_results_raises_error(self) -> None:
        """
        Test that an empty NTP results list raises a `ValidationError`.

        Verifies the Pydantic validation rule for non-empty `ntp_results`.
        """
        with pytest.raises(ValidationError) as excinfo:
            ReportInput(ntp_results=[], url_results=["url1"])
        # Check for the specific Pydantic error message
        assert any("Field 'ntp_results' cannot be empty." in error["msg"] for error in excinfo.value.errors())

    @pytest.mark.unit
    def test_empty_url_results_raises_error(self) -> None:
        """
        Test that an empty URL results list raises a `ValidationError`.

        Verifies the Pydantic validation rule for non-empty `url_results`.
        """
        with pytest.raises(ValidationError) as excinfo:
            ReportInput(ntp_results=["ntp1"], url_results=[])
        # Check for the specific Pydantic error message
        assert any("Field 'url_results' cannot be empty." in error["msg"] for error in excinfo.value.errors())

    @pytest.mark.unit
    def test_non_list_inputs_results_raises_validation_error(self) -> None:
        """
        Test that non-list inputs for results raise a `ValidationError`
        with the custom message from `ensure_list_of_strings`.
        """
        # Test case 1: ntp_results is not a list
        with pytest.raises(ValidationError) as excinfo:
            ReportInput(ntp_results="not a list", url_results=["url1"])

        error_found = False
        for error in excinfo.value.errors():
            # Expect the custom error message and 'value_error' type
            if "Input must be a list." in error["msg"] and error["type"] == "value_error":
                error_found = True
                break
        assert error_found, f"Expected 'Input must be a list.' (type value_error) but got: {excinfo.value.errors()}"

        # Test case 2: url_results is not a list
        with pytest.raises(ValidationError) as excinfo:
            ReportInput(ntp_results=["ntp1"], url_results="not a list")
        error_found = False
        for error in excinfo.value.errors():
            # Expect the custom error message and 'value_error' type
            if "Input must be a list." in error["msg"] and error["type"] == "value_error":
                error_found = True
                break
        assert error_found, f"Expected 'Input must be a list.' (type value_error) but got: {excinfo.value.errors()}"

    @pytest.mark.unit
    def test_list_with_non_string_items_raises_validation_error(self) -> None:
        """
        Test that lists containing non-string items raise a `ValidationError`.

        Verifies that the custom validator (`ensure_list_of_strings`) correctly
        flags lists with mixed or incorrect item types.
        """
        # Test case 1: ntp_results contains non-string elements
        with pytest.raises(ValidationError) as excinfo:
            ReportInput(ntp_results=["ntp1", 123], url_results=["url1"])
        assert any("All items in the list must be strings." in error["msg"] for error in excinfo.value.errors())
        assert any(error["type"] == "value_error" for error in excinfo.value.errors())

        # Test case 2: url_results contains non-string elements
        with pytest.raises(ValidationError) as excinfo:
            ReportInput(ntp_results=["ntp1"], url_results=["url1", None])
        assert any("All items in the list must be strings." in error["msg"] for error in excinfo.value.errors())
        assert any(error["type"] == "value_error" for error in excinfo.value.errors())


class TestReportGenerator:
    """
    Test suite for the `ReportGenerator` class.

    This class contains unit tests to verify the functionality of the `ReportGenerator`,
    including its initialization, directory management, and the generation of
    plain text, HTML, and PDF reports.
    """

    @pytest.mark.unit
    @pytest.mark.parametrize("app_context_fixture", ["simple"], indirect=True)
    def test_from_params_uses_explicit_reports_dir(
        self, app_context_fixture: AppContext, mocker: MockerFixture, tmp_path: Path
    ) -> None:
        """
        Test that `from_params` uses the default directory when none is configured.

        Ensures that if the `reports.directory` setting is missing from the config,
        the `ReportGenerator` falls back to its predefined default output path.
        """
        mock_ensure_dir = mocker.patch.object(ReportGenerator, "_ensure_reports_directory", return_value=None)

        generator_reports_dir = tmp_path / "another_test_reports_dir"

        generator = ReportGenerator.from_params(context=app_context_fixture, reports_dir=generator_reports_dir)
        # Assert _ensure_reports_directory was called with the correct RELATIVE path
        # The first argument is 'self', so the second is 'reports_dir'
        mock_ensure_dir.assert_called_once_with(generator_reports_dir)

        assert generator.reports_dir == generator_reports_dir

    @pytest.mark.unit
    @pytest.mark.parametrize("app_context_fixture", ["full"], indirect=True)
    def test_from_context_uses_configured_reports_dir(
        self, report_generator_from_context_instance: ReportGenerator, mocker: MockerFixture, tmp_path: Path
    ) -> None:
        """
        Test that `ReportGenerator.from_context` uses the configured directory when the context is 'full'.
        """
        mock_ensure_dir = mocker.patch.object(ReportGenerator, "_ensure_reports_directory", return_value=None)

        app_context = report_generator_from_context_instance.context

        generator = ReportGenerator.from_context(context=app_context)

        expected_path_from_config = tmp_path / "test_reports_from_config"

        mock_ensure_dir.assert_called_once_with(expected_path_from_config)
        assert generator.reports_dir == expected_path_from_config

    @pytest.mark.unit
    @pytest.mark.parametrize("app_context_fixture", ["simple"], indirect=True)
    def test_from_context_uses_default_reports_dir_if_none_in_config(
        self, report_generator_from_context_instance: ReportGenerator, mocker: MockerFixture
    ) -> None:
        """
        Test that `from_context` uses the fixed default value 'reports'
        when nothing is specified in the configuration.
        """
        mock_ensure_dir = mocker.patch.object(ReportGenerator, "_ensure_reports_directory", return_value=None)

        app_context = report_generator_from_context_instance.context

        # Create the generator manually to control the patch
        generator = ReportGenerator.from_context(context=app_context)

        expected_default_path = Path("reports")  # The fixed default value in from_context
        mock_ensure_dir.assert_called_once_with(expected_default_path)
        assert generator.reports_dir == expected_default_path

    @pytest.mark.unit
    @pytest.mark.parametrize("app_context_fixture", ["simple"], indirect=True)
    def test_ensure_reports_directory_creates_dir(
        self, report_generator_from_context_instance: ReportGenerator, tmp_path: Path
    ) -> None:
        """
        Test that `_ensure_reports_directory` creates the directory if it doesn't exist.

        Ensures that the internal helper method correctly creates the target
        output directory and its parents, and returns the verified path.
        """
        # Ensure the directory does not exist to start the test
        test_dir = tmp_path / "new_reports"
        assert not test_dir.exists()

        created_path = report_generator_from_context_instance._ensure_reports_directory(test_dir)  # noqa: SLF001
        assert created_path == test_dir
        assert test_dir.is_dir()
        assert test_dir.exists()

    @pytest.mark.unit
    @pytest.mark.parametrize("app_context_fixture", ["simple"], indirect=True)
    def test_ensure_reports_directory_raises_error_on_failure(
        self,
        app_context_fixture: AppContext,
    ) -> None:
        """
        Test that `_ensure_reports_directory` raises `DirectoryCreationError` on failure.

        Simulates an `OSError` during directory creation (e.g., permission issues)
        and asserts that the custom `DirectoryCreationError` is raised with
        appropriate messaging.
        """
        target_path = Path("/nonexistent/path_unwritable")  # A path designed to fail
        with (
            patch.object(Path, "mkdir", side_effect=OSError("Permission denied")),
            pytest.raises(DirectoryCreationError) as excinfo,
        ):
            ReportGenerator.from_params(app_context_fixture, target_path)

        assert "[mocked] Failed to create report directory /nonexistent/path_unwritable: Permission denied" in str(
            excinfo.value
        )
        assert "Permission denied" in str(excinfo.value)
        assert isinstance(excinfo.value.__cause__, OSError)

    @pytest.mark.unit
    @pytest.mark.parametrize("app_context_fixture", ["full"], indirect=True)
    def test_generate_report_creates_file(
        self, report_generator_from_context_instance: ReportGenerator, mocker: MockerFixture
    ) -> None:
        """
        Test the `generate_report` method, which creates a plain text file.

        Verifies that a report file is created at the specified path, contains
        the expected content (NTP and URL results), and that the logger
        is appropriately informed.
        """
        ntp_data = ["ntp1.example.com: OK", "ntp2.example.com: FAILED"]
        url_data = ["https://example.com: 200", "https://bad.com: 404"]
        report_input = ReportInput(ntp_results=ntp_data, url_results=url_data)

        # Mock the logger to check if the info message is logged
        mocker.patch.object(report_generator_from_context_instance.logger, "info")

        report_filename = "my_connectivity_report.txt"
        report_path = report_generator_from_context_instance.generate_report(report_input, report_filename)

        expected_path = report_generator_from_context_instance.reports_dir / report_filename
        assert report_path == expected_path
        assert report_path.exists()
        assert report_path.is_file()

        content = report_path.read_text(encoding="utf-8")
        assert "Connectivity Report" in content
        assert "NTP Results:" in content
        assert "ntp1.example.com: OK" in content
        assert "NTP Test Result 2" not in content  # Ensure no old content is present
        assert "URL Results:" in content
        assert "https://example.com: 200" in content

    @pytest.mark.unit
    def test_generate_html_report_success(
        self,
        report_generator_from_context_instance: ReportGenerator,
        mocker: MockerFixture,
        caplog_structlog: list[EventDict],
    ) -> None:
        """
        Test the successful creation of an HTML report.

        Verifies that `generate_html_report` correctly renders HTML using `ReportTemplate`,
        writes it to the expected file, and logs the successful generation.
        """
        ntp_data = ["ntp.example.com: OK"]
        url_data = ["https://example.com: 200"]

        # Mock ReportTemplate to control its rendering output
        mock_template_instance = mocker.MagicMock(spec=ReportTemplate)
        mock_template_instance.render.return_value = "<html>Mocked HTML</html>"
        mocker.patch("checkconnect.reports.report_generator.ReportTemplate", return_value=mock_template_instance)

        # Mock Path.write_text to prevent actual file system writes during this test
        mocker.patch.object(Path, "write_text")

        report_path = report_generator_from_context_instance.generate_html_report(ntp_data, url_data)

        expected_path = report_generator_from_context_instance.reports_dir / ReportGenerator.HTML_FILENAME
        assert report_path == expected_path
        # Assert ReportTemplate.render was called with the correct data
        mock_template_instance.render.assert_called_once_with("\n".join(ntp_data), "\n".join(url_data))
        # Assert Path.write_text was called with the rendered HTML and correct path
        Path.write_text.assert_called_once_with("<html>Mocked HTML</html>", encoding="utf-8")

        # Assert the logger was informed
        assert any("[mocked] HTML report generated at" in log_entry.get("event") for log_entry in caplog_structlog)

    @pytest.mark.unit
    def test_generate_html_report_missing_data_raises_error(
        self, report_generator_from_context_instance: ReportGenerator, mocker: MockerFixture
    ) -> None:
        """
        Test that `generate_html_report` raises `ValidationError` when no data is provided.

        Ensures that an attempt to generate an HTML report with empty NTP and URL
        result lists results in a `ValidationError` due to `ReportInput`'s validation.
        """
        # Mock the logger to check if the error is logged
        mocker.patch.object(report_generator_from_context_instance.logger, "error")
        mocker.patch.object(report_generator_from_context_instance.logger, "exception")

        with pytest.raises(ValidationError) as excinfo:  # Pydantic ValidationError
            report_generator_from_context_instance.generate_html_report([], [])

        assert "Field 'ntp_results' cannot be empty." in str(excinfo.value)
        assert "Field 'url_results' cannot be empty." in str(excinfo.value)
        report_generator_from_context_instance.logger.exception.assert_called_once()
        assert "Invalid report data:" in report_generator_from_context_instance.logger.exception.call_args[0][0]

    @pytest.mark.unit
    def test_generate_html_report_io_error(
        self,
        report_generator_from_context_instance: ReportGenerator,
        mocker: MockerFixture,
    ) -> None:
        """
        Test that `generate_html_report` raises `OSError` when file writing fails.

        Simulates an `OSError` during the file writing process and asserts
        that `OSError` is propagated, and the logger records an exception.
        """
        ntp_data = ["ntp.example.com: OK"]
        url_data = ["https://example.com: 200"]

        # Mock ReportTemplate so it doesn't interfere with this specific test
        mocker.patch(
            "checkconnect.reports.report_generator.ReportTemplate",
            return_value=mocker.MagicMock(
                spec=ReportTemplate, render=mocker.MagicMock(return_value="<html>Mocked Report Content</html>")
            ),
        )

        # Create an actual OSError instance to be raised and asserted against
        mock_os_error_instance = OSError("Disk full: No space left on device")

        # Mock pathlib.Path.write_text to raise an OSError when called
        mocker.patch(
            "pathlib.Path.write_text",
            side_effect=mock_os_error_instance,
        )

        # Mock the logger's exception method to assert it was called
        mocker.patch.object(report_generator_from_context_instance.logger, "exception")

        with pytest.raises(OSError, match="Disk full: No space left on device"):
            report_generator_from_context_instance.generate_html_report(ntp_data, url_data)

        report_generator_from_context_instance.logger.exception.assert_called_once_with(
            "[mocked] Error generating HTML report.",
        )

    @pytest.mark.unit
    def test_generate_pdf_report_success(
        self, report_generator_from_context_instance: ReportGenerator, mocker: MockerFixture
    ) -> None:
        """
        Test the successful creation of a PDF report.

        Verifies that `generate_pdf_report` correctly leverages `ReportTemplate`
        and `weasyprint.HTML` to generate a PDF, and logs the success.
        """
        ntp_data = ["ntp.example.com: OK"]
        url_data = ["https://example.com: 200"]

        # Mock ReportTemplate
        mock_template_instance = mocker.MagicMock(spec=ReportTemplate)
        mock_template_instance.render.return_value = "<html>Mocked HTML for PDF</html>"
        mocker.patch("checkconnect.reports.report_generator.ReportTemplate", return_value=mock_template_instance)

        # IMPORTANT: Mock the HTML class from the module where it's imported.
        # Here, that's the `checkconnect.reports.report_generator` module itself.
        mock_weasyprint_html_class = mocker.MagicMock()
        mock_weasyprint_html_instance = mocker.MagicMock()
        # When HTML() is called, it should return mock_weasyprint_html_instance
        mock_weasyprint_html_class.return_value = mock_weasyprint_html_instance
        mocker.patch("checkconnect.reports.report_generator.HTML", new=mock_weasyprint_html_class)

        mocker.patch.object(report_generator_from_context_instance.logger, "info")

        report_path = report_generator_from_context_instance.generate_pdf_report(ntp_data, url_data)

        expected_path = report_generator_from_context_instance.reports_dir / ReportGenerator.PDF_FILENAME
        assert report_path == expected_path

        # Assert ReportTemplate.render was called correctly
        mock_template_instance.render.assert_called_once_with("\n".join(ntp_data), "\n".join(url_data))
        # Assert weasyprint.HTML (the mocked constructor) was called with the rendered HTML
        mock_weasyprint_html_class.assert_called_once_with(string="<html>Mocked HTML for PDF</html>")
        # Assert write_pdf was called on the HTML instance
        mock_weasyprint_html_instance.write_pdf.assert_called_once_with(str(expected_path))
        report_generator_from_context_instance.logger.info.assert_any_call(
            "[mocked] PDF report generated at %s", str(expected_path)
        )

    @pytest.mark.unit
    def test_generate_pdf_report_missing_data_raises_error(
        self, report_generator_from_context_instance: ReportGenerator, mocker: MockerFixture
    ) -> None:
        """
        Test that `generate_pdf_report` raises `ValidationError` when no data is provided.

        Ensures that an attempt to generate a PDF report with empty NTP and URL
        result lists results in a `ValidationError` due to `ReportInput`'s validation.
        """
        mocker.patch.object(report_generator_from_context_instance.logger, "error")
        mocker.patch.object(report_generator_from_context_instance.logger, "exception")

        with pytest.raises(ValidationError) as excinfo:  # Pydantic ValidationError
            report_generator_from_context_instance.generate_pdf_report([], [])

        assert "Field 'ntp_results' cannot be empty." in str(excinfo.value)
        assert "Field 'url_results' cannot be empty." in str(excinfo.value)
        report_generator_from_context_instance.logger.exception.assert_called_once()
        assert "Invalid report data:" in report_generator_from_context_instance.logger.exception.call_args[0][0]

    @pytest.mark.unit
    def test_generate_pdf_report_error(self, report_generator_from_context_instance, mocker) -> None:
        """
        Test that `generate_pdf_report` raises a general `Exception` on PDF generation failure.

        Simulates an arbitrary `Exception` during the `weasyprint` PDF generation
        process and asserts that the `Exception` is propagated, and the logger
        records an exception.
        """
        ntp_data = ["ntp.example.com: OK"]
        url_data = ["https://example.com: 200"]

        # Mock ReportTemplate
        mocker.patch(
            "checkconnect.reports.report_generator.ReportTemplate", return_value=mocker.MagicMock(spec=ReportTemplate)
        )
        # Mock WeasyPrint HTML and make write_pdf raise an error
        mock_weasyprint_html_instance = mocker.MagicMock()
        mock_weasyprint_html_instance.__str__.return_value = "WeasyPrint error"
        mock_weasyprint_html_instance.write_pdf.side_effect = Exception("WeasyPrint error")
        mocker.patch("checkconnect.reports.report_generator.HTML", return_value=mock_weasyprint_html_instance)

        mocker.patch.object(report_generator_from_context_instance.logger, "exception")

        with pytest.raises(Exception, match="WeasyPrint error"):  # WeasyPrint raises generic Exception
            report_generator_from_context_instance.generate_pdf_report(ntp_data, url_data)

        report_generator_from_context_instance.logger.exception.assert_called_once_with(
            "[mocked] Error generating PDF report."
        )

    @pytest.mark.unit
    @pytest.mark.parametrize("app_context_fixture", ["simple"], indirect=True)
    def test_generate_reports_calls_both_html_and_pdf(
        self, report_generator_from_context_instance: ReportGenerator, mocker: MockerFixture
    ) -> None:
        """
        Test that `generate_reports` calls both `generate_html_report` and `generate_pdf_report`.

        Ensures that the orchestrating `generate_reports` method correctly
        invokes its sub-methods for both HTML and PDF generation with the
        provided data.
        """
        ntp_data = ["ntp data"]
        url_data = ["url data"]

        # Mock the internal methods to check if they are called
        mocker.patch.object(report_generator_from_context_instance, "generate_html_report")
        mocker.patch.object(report_generator_from_context_instance, "generate_pdf_report")
        mocker.patch.object(report_generator_from_context_instance.logger, "info")

        report_generator_from_context_instance.generate_reports(ntp_data, url_data)

        # Assert both methods were called with the correct arguments
        report_generator_from_context_instance.generate_html_report.assert_called_once_with(
            ntp_results=ntp_data, url_results=url_data
        )
        report_generator_from_context_instance.generate_pdf_report.assert_called_once_with(
            ntp_results=ntp_data, url_results=url_data
        )

        report_generator_from_context_instance.logger.info.assert_called_with(
            "[mocked] Generating reports in %s", report_generator_from_context_instance.reports_dir
        )

    @pytest.mark.unit
    @pytest.mark.parametrize("app_context_fixture", ["simple"], indirect=True)
    def test_module_level_generate_html_report(self, app_context_fixture: AppContext, mocker: MockerFixture) -> None:
        """
        Test the module-level `generate_html_report` function.

        Verifies that the convenience function at the module level correctly
        instantiates `ReportGenerator` and delegates to its `generate_html_report` method.
        """

        ntp_data = ["mod ntp"]
        url_data = ["mod url"]
        mock_report_generator_instance = mocker.Mock()
        mock_from_context = mocker.patch(
            "checkconnect.reports.report_generator.ReportGenerator.from_context",
            return_value=mock_report_generator_instance,
        )

        generate_html_report(app_context_fixture, ntp_data, url_data)

        # Assert: from_context was called with correct context
        mock_from_context.assert_called_once_with(context=app_context_fixture)

        mock_report_generator_instance.generate_html_report.assert_called_once_with(ntp_data, url_data)

    @pytest.mark.unit
    @pytest.mark.parametrize("app_context_fixture", ["simple"], indirect=True)
    def test_module_level_generate_pdf_report(self, app_context_fixture: AppContext, mocker: MockerFixture) -> None:
        """
        Test the module-level `generate_pdf_report` function.

        Verifies that the convenience function correctly instantiates ReportGenerator
        using from_context and calls its generate_pdf_report method.
        """

        ntp_data = ["mod ntp"]
        url_data = ["mod url"]

        # Patch from_context in the correct module
        mock_report_generator_instance = mocker.Mock()
        mock_from_context = mocker.patch(
            "checkconnect.reports.report_generator.ReportGenerator.from_context",
            return_value=mock_report_generator_instance,
        )

        # Act
        generate_pdf_report(app_context_fixture, ntp_data, url_data)

        # Assert: from_context was called with correct context
        mock_from_context.assert_called_once_with(context=app_context_fixture)

        # Assert: generate_pdf_report called with correct data
        mock_report_generator_instance.generate_pdf_report.assert_called_once_with(ntp_data, url_data)

    @pytest.mark.unit
    @pytest.mark.parametrize("app_context_fixture", ["simple"], indirect=True)
    def test_module_level_generate_reports(self, app_context_fixture: AppContext, mocker: MockerFixture) -> None:
        """
        Test that generate_reports() calls both HTML and PDF generation with the correct data.
        """
        # Arrange
        ntp_data = ["ntp1", "ntp2"]
        url_data = ["url1", "url2"]

        mock_report_generator_instance = mocker.Mock()
        mock_from_context = mocker.patch(
            "checkconnect.reports.report_generator.ReportGenerator.from_context",
            return_value=mock_report_generator_instance,
        )

        # Act
        generate_reports(app_context_fixture, ntp_data, url_data)

        # Assert: from_context called with correct context
        mock_from_context.assert_called_once_with(context=app_context_fixture)

        # Assert: both generate_* methods called correctly
        mock_report_generator_instance.generate_html_report.assert_called_once_with(ntp_data, url_data)
        mock_report_generator_instance.generate_pdf_report.assert_called_once_with(ntp_data, url_data)
